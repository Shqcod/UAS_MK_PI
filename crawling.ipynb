{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122a73bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Kompas Gaza: 100%|██████████| 300/300 [49:51<00:00,  9.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selesai! 2077 artikel disimpan ke 'kompas_gaza_articles.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "def crawl_kompas_gaza(max_pages=200):\n",
    "    articles = []\n",
    "    base_url = \"https://www.kompas.com/tag/gaza?page={}\"\n",
    "\n",
    "    for page in tqdm(range(1, max_pages + 1), desc=\"Crawling Kompas Gaza\"):\n",
    "        url = base_url.format(page)\n",
    "        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "        items = soup.select(\"div.articleItem\")\n",
    "        if not items:\n",
    "            print(f\"[!] Tidak ada artikel di halaman {page}, berhenti.\")\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            title_tag = item.select_one(\"h2.articleTitle\")\n",
    "            link_tag = item.select_one(\"a.article-link\")\n",
    "\n",
    "            if not title_tag or not link_tag:\n",
    "                continue\n",
    "\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = link_tag.get(\"href\")\n",
    "\n",
    "            # skip video\n",
    "            if \"video.kompas.com\" in link:\n",
    "                continue\n",
    "\n",
    "            # ambil isi artikel\n",
    "            try:\n",
    "                art_resp = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                art_soup = BeautifulSoup(art_resp.text, 'html.parser')\n",
    "\n",
    "                body_parts = art_soup.select(\"div.read__content p\")\n",
    "                body = \" \".join(p.get_text(strip=True) for p in body_parts)\n",
    "\n",
    "                date_tag = art_soup.select_one(\"div.read__time\")\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"Unknown\"\n",
    "\n",
    "                if len(body) > 150:\n",
    "                    articles.append({\n",
    "                        \"title\": title,\n",
    "                        \"body\": body,\n",
    "                        \"date\": date\n",
    "                    })\n",
    "\n",
    "                # delay acak biar lebih natural\n",
    "                time.sleep(random.uniform(0.4, 0.9))\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        # istirahat tiap 10 halaman\n",
    "        if page % 10 == 0:\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(\"kompas_gaza_articles.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nSelesai! {len(df)} artikel disimpan ke 'kompas_gaza_articles.csv'\")\n",
    "\n",
    "# Jalankan\n",
    "crawl_kompas_gaza(max_pages=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uasvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
